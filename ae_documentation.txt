GPU Evaluation
* Environment:
** Host (Google cloud n1-standard-8)
*** Hardware: Intel(R) Xeon(R) CPU @ 2.30GHz, Nvidia Tesla V100 SXM2 16GB
*** Software: Ubuntu 20.04, Nvidia driver version: 460.91.03, docker version 20.10.8

** Docker container
*** Software: Ubuntu 20.04, CUDA 11.1 (V11.1.105), cuDNN 8.2.1, PyTorch 1.9.0+cu111, FasterTransformer (modified on top of FasterTransformer v4.0 (commit dd4c071))
    - Make sure that nvcc is on the PATH
*** CoRa's dependencies: Z3 4.8.8.0, LLVM 9.0.0, cmake >= 3.5, G++ >= 5.0

â€‹1. Setting up the docker container
  1.Start from docker container: nvcr.io/nvidia/tensorflow:20.12-tf1-py3
  2. Install Z3 4.8.8 from prebuilt download here (https://github.com/Z3Prover/z3/releases). Download the file z3-4.8.8-x64-ubuntu-16.04.zip and move the contents of lib/ and include/ in this download to /usr/lib and /usr/include accordingly
  3. Install cuDNN 8.2.1 (for CUDA 11.x) by using the tar file installation method described here (https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-tar)
  4. Install libtinfo-dev, libxml2-dev (apt install libtinfo-dev libxml2-dev)
  5. Download pre-built llvm 9.0.0 for Ubuntu 19.04 from the LLVM download page (https://releases.llvm.org/download.html). Let the path to the root of the downloaded llvm be LLVM_PATH

2. Building and setting up CoRA
   1. Clone the repo https://github.com/pratikfegade/cora with all its submodules (git clone --recurse-submodules -j8 https://github.com/pratikfegade/cora). Make sure you're on the ragged_tensors branch Let the root of CoRa be CORA_ROOT
   2. In the file cora/config.cmake.gpu, set the variable USE_LLVM to <LLVM_PATH>/bin/llvm-config
   3. Build CoRa like so:
        mkdir build
        cp config.cmake.gpu build/config.cmake
        cd build
        cmake ..
        make -j8 tvm
    4. Finally, the compiled shared library libtvm.so should be present in the build directory
    5. Make sure that your PYTHONPATH contains ${CORA_ROOT}/python before moving on to the experiments

3. Installing PyTorch
    1. Install PyTorch 1.9.0+cu111 like so: pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html

4. Building FasterTransformer
    1. Clone the cora_benchmarks repository like so: git clone https://github.com/pratikfegade/cora_benchmarks
    2. Build faster_transformer like so:
        cd cora_benchmarks/bert_layer/faster_transformer
        mkdir build
        cd build
        cmake -DSM=xx -DCMAKE_BUILD_TYPE=Release .. where xx is the compute capability of the GPU (70 for the V100)
        make -j8

* All paths below are relative to the root of the cora_benchmarks repository

- Section 7.1: Matrix Multiplication
-- Variable-Sized Batched GEMM (vgemm)
   1. Compile vbatch_gemm/cbt, vbatch_gemm/gemm_cublas by running make in those directories
   2. Run python3 scripts/vbatch_gemm_eval.py --target cuda --max-batches 50 --out-dir results. Output will be generated in results/vbatch_gemm_results_cuda.csv.
   3. Batch size 512 may need to run separately with CoRa as it sometimes OOMs. This can be like so:
      python3 ./vbatch_gemm/tvm/vbatch_gemm.py --target cuda --batch-sizes 512 --max-batches 10 --data-file ./vbatch_gemm/data.txt.

-- Triangular Matrix Multiplication GEMM (trmm)
   1. Compile trmm/cublas by running make in those directories
   2. Run python3 scripts/trmm_eval.py --target cuda --out-dir results. Output will be generated in results/trmm_results_cuda.csv.


- Section 7.2: The Transformer Model
TODO: Subtracting prelude overheads from CUDA results

-- Table 4: Transformer encoder forward execution latencies on Nvidia V100
   1. Ensure that the line `#define OP_TIMES 1` (line 3) of the file bert_layer/faster_transformer/fastertransformer/time_map.h is commented out.
   2. Build bert_layer/fastertransformer by running `cmake -DSM=70 -DCMAKE_BUILD_TYPE=Release ..; make` in the folder bert_layer/fastertransformer/build
   3. Run python3 scripts/bert_layer_eval.py --target cuda --stdout --max-batches 50.  Output will be generated in results/bert_layer_results_cuda.csv.

   This will execute the model and generate execution latencies for a
   single encoder layer of the transformer model. During execution,
   CoRa incurs some overheads (referred to as prelude overheads)
   which, in a multi-layered model would be amortized across execution
   of multiple layers. In the paper, we report the execution latencies
   for a layer assuming a 6-layer model. In order to compute these
   execution latencies, we separately measure the prelude overheads
   and subtract 5/6th these prelude costs from the execution latencies
   measured above. The prelude overheads can be measured like so:
   1. Run python3 scripts/bert_layer_eval.py --target cuda --stdout --max-batches 50 --prelude-overhead. Output will be generated in results/bert_layer_prelude_results_cuda.csv.


-- Figures 13 and A.10: Per-operator execution time breakdown for RACE dataset at batch size 128 (Figure 13) and COLA dataset at batch size 32 (Figure A.10 in Section D.8) on the V100 GPU.
   1. Ensure that the line `#define OP_TIMES 1` (line 3) of the file bert_layer/faster_transformer/fastertransformer/time_map.h is not commented.
   2. Build bert_layer/fastertransformer by running `cmake -DSM=70 -DCMAKE_BUILD_TYPE=Release ..; make` in the folder bert_layer/fastertransformer/build.
   3. Run python3 scripts/op_times_eval.py --target cuda --out-dir results --max-batches 50 --gen-libs.  Output will be generated in results/per_op_times_results_cuda.csv.

   Just like above, we separately measure per-operator prelude
   overheads and subtract the appropritae amount from the per-operator
   ltencies measured above. These per-operator prelude overheads can
   be measured like so:
   1. Run python3 scripts/op_times_eval.py --target cuda --out-dir results --max-batches 50 --gen-libs --prelude-overhead. Output will be generated in results/per_op_times_prelude_results_cuda.csv.

-- Figure 11: MHA with and without layout change op fusion for the RACE dataset
   1. Run python3 scripts/pad_fusion_eval.py --target cuda --stdout --max-batches 50. Output will be generated in results/pad_fusion_results_cuda.csv.

-- Figure A.4: Masked Scaled Dot-Product Attention with and without padding for the attention matrix for the RACE and MNLI datasets (more discussion in Section D.3)
   1. Run python3 scripts/pad_fusion_eval.py --target cuda --max-batches 50 --out-dir results. Output will be generated in results/bert_layer_mmha_results_cuda.csv.

-- Figure A.5: Memory consumption (more discussion in Section D.5)
   1. Run python3 scripts/bert_layer_eval.py --target cuda --stdout --max-batches 50 --mem. Output will be generated in results/bert_layer_mem_results_cuda.csv.

-- Table 5: MHA evaluation on ARM CPU

- Sections 7.3 and D.6: Operation Splitting and Horizontal Fusion (AttnV in Section 7.3 and QKt in Section D.6)
  1. Run python3 scripts/bin_packed_eval.py --target cuda --stdout --max-batches 50. Output will be generated in results/bin_packed_results_cuda.csv.

- Section 7.4: CoRa Overheads
-- Prelude overheads
   1.

-- Partial padding overheads (Figure A.8)
   1.

-- Ragged tensor overheads and load hoisting (Figure A.9)
   1. Run python3 scripts/ragged_overheads_eval.py --target cuda --stdout --max-batches 1. Output will be generated in results/ragged_overheads_results_cuda.csv.

- Sections 7.5 and D.4: Evaluation Against Sparse Tensor Compilers
  1. Compile taco/{taco_bcsr_trmm, taco_bcsr_trmul, taco_csr_trmm, taco_csr_trmul, taco_csr_tradd} by running make in taco/
  2. Run python3 scripts/taco_tr_eval.py --target cuda --stdout. Output will be generated in results/taco_results_cuda.csv
